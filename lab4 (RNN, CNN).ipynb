{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cphH-e6Mq3ji"
      },
      "source": [
        "# 7 Recurrent neural networks\n",
        "In this exercise we will try a simple experiment with a recurrent neural network. One of the well-known recurrent neural network models is the so called Long short-term memory (LSTM) network. More information on LSTM can be found in the text [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
        "\n",
        "## 7.1 The MNIST dataset revisited (1)\n",
        "In one of the previous exercises the MNIST dataset was used to demonstrate the use of multilayer perceptron. Here we are going to apply a recurrent neural network to the problem of digits classification. To keep it simple, we will use a simple LSTM network that will be fed with one row of the image at a time. With each new row, it will update its states and give its prediction. What we are interested in is its prediction after the last row i.e. after it has the full information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h69cwQIWq3jm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f5ef39-92da-4793-ef48-15bf63affb23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 93076338.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 4622793.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 53754072.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 21550371.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Test Accuracy: 96.36%\n",
            "Epoch [2/10], Test Accuracy: 97.65%\n",
            "Epoch [3/10], Test Accuracy: 98.08%\n",
            "Epoch [4/10], Test Accuracy: 97.89%\n",
            "Epoch [5/10], Test Accuracy: 98.21%\n",
            "Epoch [6/10], Test Accuracy: 98.34%\n",
            "Epoch [7/10], Test Accuracy: 98.55%\n",
            "Epoch [8/10], Test Accuracy: 98.65%\n",
            "Epoch [9/10], Test Accuracy: 98.70%\n",
            "Epoch [10/10], Test Accuracy: 98.80%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Constants\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 100\n",
        "#we will feed a row at a time to the LSTM and there are 28 rows per image\n",
        "timesteps = 28\n",
        "#each row has 28 columns whose values are simultaneously passed to LSTM\n",
        "n_input = 28 # MNIST data input (img shape: 28*28)\n",
        "#the number of hidden states in the LSTM\n",
        "n_hidden = 128\n",
        "n_classes = 10\n",
        "\n",
        "# Data transformation\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Initialize the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = LSTMModel(n_input, n_hidden, 1, n_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.view(-1, timesteps, n_input).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Test the model\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in test_loader:\n",
        "                images = images.view(-1, timesteps, n_input).to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Variables to store results\n",
        "num_rows_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Training loop with varying number of rows\n",
        "for num_rows in range(1, timesteps + 1):\n",
        "    # Reset the model\n",
        "    model = LSTMModel(n_input, n_hidden, 1, n_classes).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.view(-1, timesteps, n_input)[:, :num_rows, :].to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Test the model\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.view(-1, timesteps, n_input)[:, :num_rows, :].to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Num Rows [{num_rows}], Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "        # Store results for plotting\n",
        "        num_rows_list.append(num_rows)\n",
        "        accuracy_list.append(accuracy)\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(num_rows_list, accuracy_list, marker='o')\n",
        "plt.title('Accuracy vs. Number of Rows')\n",
        "plt.xlabel('Number of Rows')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nagJwWab9MF",
        "outputId": "994a939a-75af-4ee1-9ffe-4669fc2fa186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Rows [1], Test Accuracy: 11.35%\n",
            "Num Rows [2], Test Accuracy: 11.78%\n",
            "Num Rows [3], Test Accuracy: 16.46%\n",
            "Num Rows [4], Test Accuracy: 22.28%\n",
            "Num Rows [5], Test Accuracy: 35.33%\n",
            "Num Rows [6], Test Accuracy: 52.31%\n",
            "Num Rows [7], Test Accuracy: 63.73%\n",
            "Num Rows [8], Test Accuracy: 71.17%\n",
            "Num Rows [9], Test Accuracy: 77.32%\n",
            "Num Rows [10], Test Accuracy: 82.66%\n",
            "Num Rows [11], Test Accuracy: 87.05%\n",
            "Num Rows [12], Test Accuracy: 90.45%\n",
            "Num Rows [13], Test Accuracy: 93.17%\n",
            "Num Rows [14], Test Accuracy: 94.46%\n",
            "Num Rows [15], Test Accuracy: 95.76%\n",
            "Num Rows [16], Test Accuracy: 96.66%\n",
            "Num Rows [17], Test Accuracy: 97.26%\n",
            "Num Rows [18], Test Accuracy: 97.50%\n",
            "Num Rows [19], Test Accuracy: 97.97%\n",
            "Num Rows [20], Test Accuracy: 98.21%\n",
            "Num Rows [21], Test Accuracy: 98.55%\n",
            "Num Rows [22], Test Accuracy: 98.50%\n",
            "Num Rows [23], Test Accuracy: 98.43%\n",
            "Num Rows [24], Test Accuracy: 98.69%\n",
            "Num Rows [25], Test Accuracy: 98.54%\n",
            "Num Rows [26], Test Accuracy: 98.73%\n",
            "Num Rows [27], Test Accuracy: 98.52%\n",
            "Num Rows [28], Test Accuracy: 98.74%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "iRbpYMdrq3jn"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. Study and run the code above.\n",
        "2. Draw a plot that shows the relation between the number of rows given to the network and its final accuracy on the test set.\n",
        "3. What happens if we use gradient descent instead of Adam?\n",
        "</br> Answer: gradient descent begins dropping in test accuracy after 22nd row, as shown on the image below:\n",
        "\n",
        "![download.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNpp9Ppweg3Q"
      },
      "source": [
        "# 8 Convolutional neural networks\n",
        "\n",
        "\n",
        "The goal of this exercise is to learn the basic stuff about [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN or ConvNet). In the previous exercises, the building blocks mostly included simple operations that had some kind of activations functions and each layer was usually fully connected to the previous one. CNNs take into account the spatial nature of the input data, e.g. an image, and they process it by applying one or more  [kernels](https://en.wikipedia.org/wiki/Kernel_%28image_processing%29). In the case of images, this processing i.e. convolving is also known as filtering. The results of processing the input with a single kernel will be a single channel, but usually a convolutional layer involves more kernels producing more channels. These channels are often called **feature maps** because each kernel is specialized for extraction of a certain kind of features from the input. These feature maps are then combined into a single tensor that can be viewed as an image with multiple channels that can be then passed to further convolutional layers.\n",
        "\n",
        "For example, if the input consists of a grayscale image i.e. an image with only one channel and a $5\\times 5$ kernel is applied, the result is a single feature map. The borders of the input image are usually padded with zeros to ensure that the resulting feature maps has the same number of rows and columns as the input image.\n",
        "\n",
        "If the input consists of a color image i.e. an image with three channels and a $5\\times 5$ kernel is applied, what will actually be applied is an $5\\times 5\\times 3$ kernel that will simultaneously process all three channels and the result will again be a single feature map. However, if e.g. 16 several kernels are applied, then the result will be 16 feature maps. Should they be passed to another convolutional layer, **each** of its kernels would simultaneously process **all** feature maps so their sizes would be e.g. $3\\times 3\\times 16$ or $5\\times 5\\times 16$ where 16 is used to reach all feature maps simultaneously.\n",
        "\n",
        "The convolution is usually followed by applying an element-wise non-linear operation to each of the values in the feature maps. Finally, what often follows is the summarization i.e. pooling of the information in the feature maps to reduce the spatial dimensions and keep only the most important information. A common approach used here is the so called max pooling. It is a non-linear downsampling where the input is divided into a set of non-overlapping rectangles and for each of them only the maximum value inside of it is kept.\n",
        "\n",
        "![Model of a neuron](data:image/png;base64,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)\n",
        "<center>Figure 1. Max pooling with $2\\times 2$ rectangles (taken from [Wikipedia](https://en.wikipedia.org/wiki/File:Max_pooling.png)).</center>\n",
        "\n",
        "What usually follows after several convolutional layers is putting the values of all feature maps into a single vector, which is then passed further to fully connected or other kinds of layers.\n",
        "\n",
        "The number of parameters in the convolutional depends on the number of feature maps and the sizes of the kernels. For example, if a convolutional layer with 32 kernels of nominal size $3\\times 3$ receives 16 feature maps on its input, it will require $16\\times 3\\times 3\\times 32+32$ where the last 32 parameters refer to the kernel biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms42xQeSVkOO"
      },
      "source": [
        "### Google Colab preliminaries\n",
        "\n",
        "Upload the zipped cnn_img folder to Google Colab and ensure the paths in the notebook are adjusted accordingly.\n",
        "\n",
        "If the notebook is not run on Google Colab, skip the following commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kmh4zv-2rzmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2fa997b-f3ab-472f-dbd8-9ea4dac02dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j8Wz1GQ5ZUlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18beeced-b215-4dea-a160-5d8907571b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/cnn_img.zip\n",
            "   creating: cnn_img/\n",
            " extracting: cnn_img/badger.jpg      \n",
            "  inflating: cnn_img/can.jpg         \n",
            "   creating: cnn_img/healthy/\n",
            "  inflating: cnn_img/healthy/0.jpg   \n",
            "  inflating: cnn_img/healthy/1.jpg   \n",
            "  inflating: cnn_img/healthy/10.jpg  \n",
            "  inflating: cnn_img/healthy/11.jpg  \n",
            "  inflating: cnn_img/healthy/12.jpg  \n",
            "  inflating: cnn_img/healthy/13.jpg  \n",
            "  inflating: cnn_img/healthy/14.jpg  \n",
            "  inflating: cnn_img/healthy/15.jpg  \n",
            "  inflating: cnn_img/healthy/16.jpg  \n",
            "  inflating: cnn_img/healthy/17.jpg  \n",
            "  inflating: cnn_img/healthy/18.jpg  \n",
            "  inflating: cnn_img/healthy/19.jpg  \n",
            "  inflating: cnn_img/healthy/2.jpg   \n",
            "  inflating: cnn_img/healthy/20.jpg  \n",
            "  inflating: cnn_img/healthy/21.jpg  \n",
            "  inflating: cnn_img/healthy/22.jpg  \n",
            "  inflating: cnn_img/healthy/23.jpg  \n",
            "  inflating: cnn_img/healthy/24.jpg  \n",
            "  inflating: cnn_img/healthy/25.jpg  \n",
            "  inflating: cnn_img/healthy/26.jpg  \n",
            "  inflating: cnn_img/healthy/27.jpg  \n",
            "  inflating: cnn_img/healthy/28.jpg  \n",
            "  inflating: cnn_img/healthy/29.jpg  \n",
            "  inflating: cnn_img/healthy/3.jpg   \n",
            "  inflating: cnn_img/healthy/30.jpg  \n",
            "  inflating: cnn_img/healthy/31.jpg  \n",
            "  inflating: cnn_img/healthy/32.jpg  \n",
            "  inflating: cnn_img/healthy/33.jpg  \n",
            "  inflating: cnn_img/healthy/34.jpg  \n",
            "  inflating: cnn_img/healthy/35.jpg  \n",
            "  inflating: cnn_img/healthy/36.jpg  \n",
            "  inflating: cnn_img/healthy/37.jpg  \n",
            "  inflating: cnn_img/healthy/38.jpg  \n",
            "  inflating: cnn_img/healthy/39.jpg  \n",
            "  inflating: cnn_img/healthy/4.jpg   \n",
            "  inflating: cnn_img/healthy/40.jpg  \n",
            "  inflating: cnn_img/healthy/41.jpg  \n",
            "  inflating: cnn_img/healthy/42.jpg  \n",
            "  inflating: cnn_img/healthy/43.jpg  \n",
            "  inflating: cnn_img/healthy/44.jpg  \n",
            "  inflating: cnn_img/healthy/45.jpg  \n",
            "  inflating: cnn_img/healthy/46.jpg  \n",
            "  inflating: cnn_img/healthy/47.jpg  \n",
            "  inflating: cnn_img/healthy/48.jpg  \n",
            "  inflating: cnn_img/healthy/49.jpg  \n",
            "  inflating: cnn_img/healthy/5.jpg   \n",
            "  inflating: cnn_img/healthy/50.jpg  \n",
            "  inflating: cnn_img/healthy/51.jpg  \n",
            "  inflating: cnn_img/healthy/52.jpg  \n",
            "  inflating: cnn_img/healthy/53.jpg  \n",
            "  inflating: cnn_img/healthy/54.jpg  \n",
            "  inflating: cnn_img/healthy/55.jpg  \n",
            "  inflating: cnn_img/healthy/56.jpg  \n",
            "  inflating: cnn_img/healthy/57.jpg  \n",
            "  inflating: cnn_img/healthy/58.jpg  \n",
            "  inflating: cnn_img/healthy/59.jpg  \n",
            "  inflating: cnn_img/healthy/6.jpg   \n",
            "  inflating: cnn_img/healthy/60.jpg  \n",
            "  inflating: cnn_img/healthy/61.jpg  \n",
            "  inflating: cnn_img/healthy/62.jpg  \n",
            "  inflating: cnn_img/healthy/63.jpg  \n",
            "  inflating: cnn_img/healthy/64.jpg  \n",
            "  inflating: cnn_img/healthy/65.jpg  \n",
            "  inflating: cnn_img/healthy/66.jpg  \n",
            "  inflating: cnn_img/healthy/67.jpg  \n",
            "  inflating: cnn_img/healthy/68.jpg  \n",
            "  inflating: cnn_img/healthy/69.jpg  \n",
            "  inflating: cnn_img/healthy/7.jpg   \n",
            "  inflating: cnn_img/healthy/70.jpg  \n",
            "  inflating: cnn_img/healthy/71.jpg  \n",
            "  inflating: cnn_img/healthy/72.jpg  \n",
            "  inflating: cnn_img/healthy/73.jpg  \n",
            "  inflating: cnn_img/healthy/74.jpg  \n",
            "  inflating: cnn_img/healthy/75.jpg  \n",
            "  inflating: cnn_img/healthy/76.jpg  \n",
            "  inflating: cnn_img/healthy/77.jpg  \n",
            "  inflating: cnn_img/healthy/78.jpg  \n",
            "  inflating: cnn_img/healthy/79.jpg  \n",
            "  inflating: cnn_img/healthy/8.jpg   \n",
            "  inflating: cnn_img/healthy/80.jpg  \n",
            "  inflating: cnn_img/healthy/81.jpg  \n",
            "  inflating: cnn_img/healthy/82.jpg  \n",
            "  inflating: cnn_img/healthy/83.jpg  \n",
            "  inflating: cnn_img/healthy/84.jpg  \n",
            "  inflating: cnn_img/healthy/85.jpg  \n",
            "  inflating: cnn_img/healthy/86.jpg  \n",
            "  inflating: cnn_img/healthy/87.jpg  \n",
            "  inflating: cnn_img/healthy/88.jpg  \n",
            "  inflating: cnn_img/healthy/89.jpg  \n",
            "  inflating: cnn_img/healthy/9.jpg   \n",
            "  inflating: cnn_img/healthy/90.jpg  \n",
            "  inflating: cnn_img/healthy/91.jpg  \n",
            "  inflating: cnn_img/healthy/92.jpg  \n",
            "  inflating: cnn_img/healthy/93.jpg  \n",
            "  inflating: cnn_img/healthy/94.jpg  \n",
            "  inflating: cnn_img/healthy/95.jpg  \n",
            "  inflating: cnn_img/healthy/96.jpg  \n",
            "  inflating: cnn_img/healthy/97.jpg  \n",
            "  inflating: cnn_img/healthy/98.jpg  \n",
            "  inflating: cnn_img/healthy/99.jpg  \n",
            "  inflating: cnn_img/max_pooling_2x2.png  \n",
            "   creating: cnn_img/pincers/\n",
            "  inflating: cnn_img/pincers/0.jpg   \n",
            "  inflating: cnn_img/pincers/1.jpg   \n",
            "  inflating: cnn_img/pincers/10.jpg  \n",
            "  inflating: cnn_img/pincers/11.jpg  \n",
            "  inflating: cnn_img/pincers/12.jpg  \n",
            "  inflating: cnn_img/pincers/13.jpg  \n",
            "  inflating: cnn_img/pincers/14.jpg  \n",
            "  inflating: cnn_img/pincers/15.jpg  \n",
            "  inflating: cnn_img/pincers/16.jpg  \n",
            "  inflating: cnn_img/pincers/17.jpg  \n",
            "  inflating: cnn_img/pincers/18.jpg  \n",
            "  inflating: cnn_img/pincers/19.jpg  \n",
            "  inflating: cnn_img/pincers/2.jpg   \n",
            "  inflating: cnn_img/pincers/20.jpg  \n",
            "  inflating: cnn_img/pincers/21.jpg  \n",
            "  inflating: cnn_img/pincers/22.jpg  \n",
            "  inflating: cnn_img/pincers/23.jpg  \n",
            "  inflating: cnn_img/pincers/24.jpg  \n",
            "  inflating: cnn_img/pincers/25.jpg  \n",
            "  inflating: cnn_img/pincers/26.jpg  \n",
            "  inflating: cnn_img/pincers/27.jpg  \n",
            "  inflating: cnn_img/pincers/28.jpg  \n",
            "  inflating: cnn_img/pincers/29.jpg  \n",
            "  inflating: cnn_img/pincers/3.jpg   \n",
            "  inflating: cnn_img/pincers/30.jpg  \n",
            "  inflating: cnn_img/pincers/31.jpg  \n",
            "  inflating: cnn_img/pincers/32.jpg  \n",
            "  inflating: cnn_img/pincers/33.jpg  \n",
            "  inflating: cnn_img/pincers/34.jpg  \n",
            "  inflating: cnn_img/pincers/35.jpg  \n",
            "  inflating: cnn_img/pincers/36.jpg  \n",
            "  inflating: cnn_img/pincers/37.jpg  \n",
            "  inflating: cnn_img/pincers/38.jpg  \n",
            "  inflating: cnn_img/pincers/39.jpg  \n",
            "  inflating: cnn_img/pincers/4.jpg   \n",
            "  inflating: cnn_img/pincers/40.jpg  \n",
            "  inflating: cnn_img/pincers/41.jpg  \n",
            "  inflating: cnn_img/pincers/42.jpg  \n",
            "  inflating: cnn_img/pincers/43.jpg  \n",
            "  inflating: cnn_img/pincers/44.jpg  \n",
            "  inflating: cnn_img/pincers/45.jpg  \n",
            "  inflating: cnn_img/pincers/46.jpg  \n",
            "  inflating: cnn_img/pincers/47.jpg  \n",
            "  inflating: cnn_img/pincers/48.jpg  \n",
            "  inflating: cnn_img/pincers/49.jpg  \n",
            "  inflating: cnn_img/pincers/5.jpg   \n",
            "  inflating: cnn_img/pincers/6.jpg   \n",
            "  inflating: cnn_img/pincers/7.jpg   \n",
            "  inflating: cnn_img/pincers/8.jpg   \n",
            "  inflating: cnn_img/pincers/9.jpg   \n",
            " extracting: cnn_img/pineapple.jpg   \n",
            " extracting: cnn_img/rabbit.jpg      \n",
            "   creating: cnn_img/scissors/\n",
            "  inflating: cnn_img/scissors/0.jpg  \n",
            "  inflating: cnn_img/scissors/1.jpg  \n",
            "  inflating: cnn_img/scissors/10.jpg  \n",
            "  inflating: cnn_img/scissors/11.jpg  \n",
            "  inflating: cnn_img/scissors/12.jpg  \n",
            "  inflating: cnn_img/scissors/13.jpg  \n",
            "  inflating: cnn_img/scissors/14.jpg  \n",
            "  inflating: cnn_img/scissors/15.jpg  \n",
            "  inflating: cnn_img/scissors/16.jpg  \n",
            "  inflating: cnn_img/scissors/17.jpg  \n",
            "  inflating: cnn_img/scissors/18.jpg  \n",
            "  inflating: cnn_img/scissors/19.jpg  \n",
            "  inflating: cnn_img/scissors/2.jpg  \n",
            "  inflating: cnn_img/scissors/20.jpg  \n",
            "  inflating: cnn_img/scissors/21.jpg  \n",
            "  inflating: cnn_img/scissors/22.jpg  \n",
            "  inflating: cnn_img/scissors/23.jpg  \n",
            "  inflating: cnn_img/scissors/24.jpg  \n",
            "  inflating: cnn_img/scissors/25.jpg  \n",
            "  inflating: cnn_img/scissors/26.jpg  \n",
            "  inflating: cnn_img/scissors/27.jpg  \n",
            "  inflating: cnn_img/scissors/28.jpg  \n",
            "  inflating: cnn_img/scissors/29.jpg  \n",
            "  inflating: cnn_img/scissors/3.jpg  \n",
            "  inflating: cnn_img/scissors/30.jpg  \n",
            "  inflating: cnn_img/scissors/31.jpg  \n",
            "  inflating: cnn_img/scissors/32.jpg  \n",
            "  inflating: cnn_img/scissors/33.jpg  \n",
            "  inflating: cnn_img/scissors/34.jpg  \n",
            "  inflating: cnn_img/scissors/35.jpg  \n",
            "  inflating: cnn_img/scissors/36.jpg  \n",
            "  inflating: cnn_img/scissors/37.jpg  \n",
            "  inflating: cnn_img/scissors/38.jpg  \n",
            "  inflating: cnn_img/scissors/39.jpg  \n",
            "  inflating: cnn_img/scissors/4.jpg  \n",
            "  inflating: cnn_img/scissors/40.jpg  \n",
            "  inflating: cnn_img/scissors/41.jpg  \n",
            "  inflating: cnn_img/scissors/42.jpg  \n",
            "  inflating: cnn_img/scissors/43.jpg  \n",
            "  inflating: cnn_img/scissors/44.jpg  \n",
            "  inflating: cnn_img/scissors/45.jpg  \n",
            "  inflating: cnn_img/scissors/46.jpg  \n",
            "  inflating: cnn_img/scissors/47.jpg  \n",
            "  inflating: cnn_img/scissors/48.jpg  \n",
            "  inflating: cnn_img/scissors/49.jpg  \n",
            "  inflating: cnn_img/scissors/5.jpg  \n",
            "  inflating: cnn_img/scissors/6.jpg  \n",
            "  inflating: cnn_img/scissors/7.jpg  \n",
            "  inflating: cnn_img/scissors/8.jpg  \n",
            "  inflating: cnn_img/scissors/9.jpg  \n",
            "  inflating: cnn_img/sundial.jpg     \n",
            "   creating: cnn_img/unhealthy/\n",
            "  inflating: cnn_img/unhealthy/0.jpg  \n",
            "  inflating: cnn_img/unhealthy/1.jpg  \n",
            "  inflating: cnn_img/unhealthy/10.jpg  \n",
            "  inflating: cnn_img/unhealthy/11.jpg  \n",
            "  inflating: cnn_img/unhealthy/12.jpg  \n",
            "  inflating: cnn_img/unhealthy/13.jpg  \n",
            "  inflating: cnn_img/unhealthy/14.jpg  \n",
            "  inflating: cnn_img/unhealthy/15.jpg  \n",
            "  inflating: cnn_img/unhealthy/16.jpg  \n",
            "  inflating: cnn_img/unhealthy/17.jpg  \n",
            "  inflating: cnn_img/unhealthy/18.jpg  \n",
            "  inflating: cnn_img/unhealthy/19.jpg  \n",
            "  inflating: cnn_img/unhealthy/2.jpg  \n",
            "  inflating: cnn_img/unhealthy/20.jpg  \n",
            "  inflating: cnn_img/unhealthy/21.jpg  \n",
            "  inflating: cnn_img/unhealthy/22.jpg  \n",
            "  inflating: cnn_img/unhealthy/23.jpg  \n",
            "  inflating: cnn_img/unhealthy/24.jpg  \n",
            "  inflating: cnn_img/unhealthy/25.jpg  \n",
            "  inflating: cnn_img/unhealthy/26.jpg  \n",
            "  inflating: cnn_img/unhealthy/27.jpg  \n",
            "  inflating: cnn_img/unhealthy/28.jpg  \n",
            "  inflating: cnn_img/unhealthy/29.jpg  \n",
            "  inflating: cnn_img/unhealthy/3.jpg  \n",
            "  inflating: cnn_img/unhealthy/30.jpg  \n",
            "  inflating: cnn_img/unhealthy/31.jpg  \n",
            "  inflating: cnn_img/unhealthy/32.jpg  \n",
            "  inflating: cnn_img/unhealthy/33.jpg  \n",
            "  inflating: cnn_img/unhealthy/34.jpg  \n",
            "  inflating: cnn_img/unhealthy/35.jpg  \n",
            "  inflating: cnn_img/unhealthy/36.jpg  \n",
            "  inflating: cnn_img/unhealthy/37.jpg  \n",
            "  inflating: cnn_img/unhealthy/38.jpg  \n",
            "  inflating: cnn_img/unhealthy/39.jpg  \n",
            "  inflating: cnn_img/unhealthy/4.jpg  \n",
            "  inflating: cnn_img/unhealthy/40.jpg  \n",
            "  inflating: cnn_img/unhealthy/41.jpg  \n",
            "  inflating: cnn_img/unhealthy/42.jpg  \n",
            "  inflating: cnn_img/unhealthy/43.jpg  \n",
            "  inflating: cnn_img/unhealthy/44.jpg  \n",
            "  inflating: cnn_img/unhealthy/45.jpg  \n",
            "  inflating: cnn_img/unhealthy/46.jpg  \n",
            "  inflating: cnn_img/unhealthy/47.jpg  \n",
            "  inflating: cnn_img/unhealthy/48.jpg  \n",
            "  inflating: cnn_img/unhealthy/49.jpg  \n",
            "  inflating: cnn_img/unhealthy/5.jpg  \n",
            "  inflating: cnn_img/unhealthy/50.jpg  \n",
            "  inflating: cnn_img/unhealthy/51.jpg  \n",
            "  inflating: cnn_img/unhealthy/52.jpg  \n",
            "  inflating: cnn_img/unhealthy/53.jpg  \n",
            "  inflating: cnn_img/unhealthy/54.jpg  \n",
            "  inflating: cnn_img/unhealthy/55.jpg  \n",
            "  inflating: cnn_img/unhealthy/56.jpg  \n",
            "  inflating: cnn_img/unhealthy/57.jpg  \n",
            "  inflating: cnn_img/unhealthy/58.jpg  \n",
            "  inflating: cnn_img/unhealthy/59.jpg  \n",
            "  inflating: cnn_img/unhealthy/6.jpg  \n",
            "  inflating: cnn_img/unhealthy/60.jpg  \n",
            "  inflating: cnn_img/unhealthy/61.jpg  \n",
            "  inflating: cnn_img/unhealthy/62.jpg  \n",
            "  inflating: cnn_img/unhealthy/63.jpg  \n",
            "  inflating: cnn_img/unhealthy/64.jpg  \n",
            "  inflating: cnn_img/unhealthy/65.jpg  \n",
            "  inflating: cnn_img/unhealthy/66.jpg  \n",
            "  inflating: cnn_img/unhealthy/67.jpg  \n",
            "  inflating: cnn_img/unhealthy/68.jpg  \n",
            "  inflating: cnn_img/unhealthy/69.jpg  \n",
            "  inflating: cnn_img/unhealthy/7.jpg  \n",
            "  inflating: cnn_img/unhealthy/70.jpg  \n",
            "  inflating: cnn_img/unhealthy/71.jpg  \n",
            "  inflating: cnn_img/unhealthy/72.jpg  \n",
            "  inflating: cnn_img/unhealthy/73.jpg  \n",
            "  inflating: cnn_img/unhealthy/74.jpg  \n",
            "  inflating: cnn_img/unhealthy/75.jpg  \n",
            "  inflating: cnn_img/unhealthy/76.jpg  \n",
            "  inflating: cnn_img/unhealthy/77.jpg  \n",
            "  inflating: cnn_img/unhealthy/78.jpg  \n",
            "  inflating: cnn_img/unhealthy/79.jpg  \n",
            "  inflating: cnn_img/unhealthy/8.jpg  \n",
            "  inflating: cnn_img/unhealthy/80.jpg  \n",
            "  inflating: cnn_img/unhealthy/81.jpg  \n",
            "  inflating: cnn_img/unhealthy/82.jpg  \n",
            "  inflating: cnn_img/unhealthy/83.jpg  \n",
            "  inflating: cnn_img/unhealthy/84.jpg  \n",
            "  inflating: cnn_img/unhealthy/85.jpg  \n",
            "  inflating: cnn_img/unhealthy/86.jpg  \n",
            "  inflating: cnn_img/unhealthy/87.jpg  \n",
            "  inflating: cnn_img/unhealthy/88.jpg  \n",
            "  inflating: cnn_img/unhealthy/89.jpg  \n",
            "  inflating: cnn_img/unhealthy/9.jpg  \n",
            "  inflating: cnn_img/unhealthy/90.jpg  \n",
            "  inflating: cnn_img/unhealthy/91.jpg  \n",
            "  inflating: cnn_img/unhealthy/92.jpg  \n",
            "  inflating: cnn_img/unhealthy/93.jpg  \n",
            "  inflating: cnn_img/unhealthy/94.jpg  \n",
            "  inflating: cnn_img/unhealthy/95.jpg  \n",
            "  inflating: cnn_img/unhealthy/96.jpg  \n",
            "  inflating: cnn_img/unhealthy/97.jpg  \n",
            "  inflating: cnn_img/unhealthy/98.jpg  \n",
            "  inflating: cnn_img/unhealthy/99.jpg  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/gdrive/MyDrive/cnn_img.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJdpRk2gV9q4"
      },
      "source": [
        "## 8.1 The MNIST dataset revisited (2)\n",
        "In one of the previous exercises the MNIST dataset was used to demonstrate the use of multilayer perceptron. Here we are going to apply a convolutional neural network to the digit classification problem. We will use the following layers to build our model:\n",
        "\n",
        "* [torch.nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
        "* [torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
        "* [torch.nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)\n",
        "* [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
        "\n",
        "The [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) layer has the same effect as the fully connected layer, a matrix multiplication that was used in the previous exercise with the MNIST dataset.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Study and run the code below. How is the accuracy compared to the ones obtained in the previous exerises with MNIST?\n",
        "2. Try to change the number and size of convolutional and fully connected layers. What has the greatest impact on the accuracy? For each network architecture configuration calculate the number of trainable parameters.\n",
        "</br>Answer: the size and number of convolutional layers has the greatest impact on accuracy, whereas the fully connected layer not so much (increasing to 256 neurons results in mere 0.2% drop accuracy). Below in code are numbers of trainable params in different models.\n",
        "3. What happens to the accuracy if another [non-linearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) is used instead of ReLU? Experiment with at least two different activation functions.\n",
        "</br>Answer: sigmoid activation function yields somewhat lower final accuracy than ReLU (sigmoid 98.8%, ReLU 99.3%). Tanh activation function yields 98.6% final accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m_1TWwNif8WE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NK79DP85edIb"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "  # Method that defines the layers and other components of a model\n",
        "  def __init__(self,\n",
        "               input_channels,\n",
        "               n_channels_1,\n",
        "               n_channels_2,\n",
        "               n_fully_connected,\n",
        "               n_classes,\n",
        "               kernel_size\n",
        "               ):\n",
        "\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels=input_channels,\n",
        "                           out_channels=n_channels_1,\n",
        "                           kernel_size=kernel_size,\n",
        "                           padding='same'\n",
        "                           )\n",
        "\n",
        "    self.relu1 = nn.ReLU()\n",
        "\n",
        "    self.maxpool1 = nn.MaxPool2d((2,2))\n",
        "\n",
        "    self.conv2 = nn.Conv2d(in_channels=n_channels_1,\n",
        "                           out_channels=n_channels_2,\n",
        "                           kernel_size=kernel_size,\n",
        "                           padding='same'\n",
        "                           )\n",
        "\n",
        "    self.relu2 = nn.ReLU()\n",
        "\n",
        "    self.maxpool2 = nn.MaxPool2d((2,2))\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=7*7*n_channels_2, out_features=n_fully_connected, bias=True)\n",
        "\n",
        "    self.relu3 = nn.ReLU()\n",
        "\n",
        "    self.fc2 = nn.Linear(in_features=n_fully_connected, out_features=n_classes, bias=True)\n",
        "\n",
        "  # Method where the computation gets done\n",
        "  def forward(self, x):\n",
        "\n",
        "    # First convolutional layer\n",
        "    # We will apply n_channels_1 kernels of size kernel_size X kernel_size\n",
        "    # We are padding the input in order for the result to have the same number of rows and columns\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    # Applying the non-linearity\n",
        "    x = self.relu1(x)\n",
        "\n",
        "    # and max pooling again, now each feature map will be of size 7 X 7\n",
        "    x = self.maxpool1(x)\n",
        "\n",
        "    # Second convolutional layer\n",
        "    # We will apply n_channels_2 kernels of size kernel_size X kernel_size\n",
        "    x = self.conv2(x)\n",
        "\n",
        "    # again, we apply the non-linearity\n",
        "    x = self.relu2(x)\n",
        "\n",
        "    # and max pooling again, now each feature map will be of size 7 X 7\n",
        "    x = self.maxpool2(x)\n",
        "\n",
        "    # Flatten all dimensions except the batch\n",
        "    x = torch.flatten(x, 1)\n",
        "\n",
        "    # Fully connected layer\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    # and again, we apply the non-linearity\n",
        "    x = self.relu3(x)\n",
        "\n",
        "    # Non-linearity\n",
        "    pred_logits = self.fc2(x)\n",
        "\n",
        "    return pred_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FFJtOH6oNfLR"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, device, train_dataloader, optimizer, epoch):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in tqdm.tqdm(train_dataloader):\n",
        "\n",
        "      # Every data instance is an input image + label pair\n",
        "      images, labels = batch\n",
        "\n",
        "      # It is necessary to have both the model, and the data on the same device, either CPU or GPU, for the model to process data.\n",
        "      # Data on CPU and model on GPU, or vice-versa, will result in a Runtime error.\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      # Zero your gradients for every batch\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Make predictions for this batch\n",
        "      pred_logits = model(images)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss = loss_fn(pred_logits, labels)\n",
        "\n",
        "      # Calculates the backward gradients over the learning weights\n",
        "      loss.backward()\n",
        "\n",
        "      # Tells the optimizer to perform one learning step\n",
        "      # Adjust the model�s learning weights based on the observed gradients for this batch\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "\n",
        "    # Print epoch's average loss\n",
        "    print(\"Epoch {} - Training loss: {}\".format(epoch+1, train_loss/len(train_dataloader)))\n",
        "\n",
        "\n",
        "def evaluation(model, device, test_dataloader, epoch):\n",
        "\n",
        "    # Sets layers like dropout and batch normalization to evaluation mode before running inference\n",
        "    # Failing to do this will yield inconsistent inference results\n",
        "    model.eval()\n",
        "\n",
        "    test_accuracy = 0.0\n",
        "\n",
        "    # Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward().\n",
        "    # It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n",
        "    with torch.no_grad():\n",
        "\n",
        "      for batch in tqdm.tqdm(test_dataloader):\n",
        "\n",
        "        images, labels = batch\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        pred_logits = model(images)\n",
        "\n",
        "        probabilities = torch.nn.functional.softmax(pred_logits, dim=1)\n",
        "\n",
        "        # Find the index of the highest probability\n",
        "        predictions = probabilities.argmax(dim=1)\n",
        "\n",
        "        # Caluculate average batch accuracy\n",
        "        batch_accuracy = torch.mean((predictions == labels).float())\n",
        "\n",
        "        test_accuracy += batch_accuracy\n",
        "\n",
        "      print(\"Epoch {} - Accuracy: {}\".format(epoch+1, test_accuracy/len(test_dataloader)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xpUS9zIEoZ4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da3ef8a-bd93-46ab-c5b1-b78915df1060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 459786243.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 110930122.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 170007163.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 20116714.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Architecture configs\n",
        "input_channels=1\n",
        "n_channels_1=32\n",
        "n_channels_2=64\n",
        "n_classes=10\n",
        "n_fully_connected=128\n",
        "kernel_size=5\n",
        "\n",
        "# Training configs\n",
        "training_epochs_count = 5\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "display_step=1\n",
        "\n",
        "# Model\n",
        "model = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n",
        "\n",
        "# Move model to GPU if possible\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Augmentations\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "# Stores the samples and their corresponding labels\n",
        "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
        "\n",
        "# Wraps an iterable around the Dataset to enable easy access to the samples.\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dPXNNzNl7Lzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805cfaf9-e879-4196-8709-676d27338bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:17<00:00, 52.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Training loss: 0.12200606472394082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 69.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Accuracy: 0.983877420425415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:16<00:00, 56.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Training loss: 0.0408337629613477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 54.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Accuracy: 0.9894506335258484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:17<00:00, 54.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Training loss: 0.027110578998761833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 75.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Accuracy: 0.9883559346199036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:16<00:00, 57.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Training loss: 0.02005298552901912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 60.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Accuracy: 0.9915406107902527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:18<00:00, 50.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Training loss: 0.015125298549828043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 73.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Accuracy: 0.9925358295440674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Define loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training\n",
        "for epoch in range(training_epochs_count):\n",
        "\n",
        "  train_epoch(model, device, train_dataloader, optimizer, epoch)\n",
        "\n",
        "  if (epoch + 1) % display_step == 0:\n",
        "\n",
        "    evaluation(model, device, test_dataloader, epoch)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ],
      "metadata": {
        "id": "6Uc-1Bnu3vQJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_channels=1\n",
        "n_channels_1=32\n",
        "n_channels_2=64\n",
        "n_classes=10\n",
        "n_fully_connected=128\n",
        "kernel_size=5\n",
        "\n",
        "# Original CNN\n",
        "original_cnn = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n",
        "print(\"Original CNN - Trainable Parameters:\", count_parameters(original_cnn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMmcpH7U4AsZ",
        "outputId": "dcd2f580-fc88-468d-93ca-ee59e85f7954"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original CNN - Trainable Parameters: 454922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_channels_1 = 64\n",
        "n_channels_2 = 128\n",
        "modified_cnn1 = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n",
        "print(\"CNN with increased conv size - Trainable Parameters:\", count_parameters(modified_cnn1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhfm2ts331Rg",
        "outputId": "49d386ad-ec25-477d-c2d0-3dbb7a484e09"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN with increased conv size - Trainable Parameters: 1815050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlteredCNN(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_channels,\n",
        "                 n_channels_1,\n",
        "                 n_channels_2,\n",
        "                 n_channels_3,  # Added new convolutional layer\n",
        "                 n_fully_connected,\n",
        "                 n_classes,\n",
        "                 kernel_size\n",
        "                 ):\n",
        "\n",
        "        super(AlteredCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_channels,\n",
        "                               out_channels=n_channels_1,\n",
        "                               kernel_size=kernel_size,\n",
        "                               padding='same'\n",
        "                               )\n",
        "\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.maxpool1 = nn.MaxPool2d((2, 2))\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=n_channels_1,\n",
        "                               out_channels=n_channels_2,\n",
        "                               kernel_size=kernel_size,\n",
        "                               padding='same'\n",
        "                               )\n",
        "\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.maxpool2 = nn.MaxPool2d((2, 2))\n",
        "\n",
        "        # New convolutional layer\n",
        "        self.conv3 = nn.Conv2d(in_channels=n_channels_2,\n",
        "                               out_channels=n_channels_3,\n",
        "                               kernel_size=kernel_size,\n",
        "                               padding='same'\n",
        "                               )\n",
        "\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.maxpool3 = nn.MaxPool2d((2, 2))\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=3 * 3 * n_channels_3, out_features=n_fully_connected, bias=True)\n",
        "\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features=n_fully_connected, out_features=n_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "        # New convolutional layer\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.maxpool3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "\n",
        "        pred_logits = self.fc2(x)\n",
        "\n",
        "        return pred_logits\n"
      ],
      "metadata": {
        "id": "ocIrE9rw55VD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_channels_1 = 32\n",
        "n_channels_2 = 64\n",
        "n_channels_3 = 64\n",
        "modified_cnn1 = AlteredCNN(input_channels, n_channels_1, n_channels_2, n_channels_3, n_fully_connected, n_classes, kernel_size)\n",
        "print(\"CNN with added conv layer - Trainable Parameters:\", count_parameters(modified_cnn1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6w5oMwZ6Y0_",
        "outputId": "23d8c22b-444b-4d06-9e33-7380a40c3b40"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN with added conv layer - Trainable Parameters: 304842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsU9MOc8QmDe"
      },
      "source": [
        "## 8.2 Image classification\n",
        "Image classification is a challenging computer vision problem with the best-known competition being [The ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](http://www.image-net.org/challenges/LSVRC/), which includes the ImageNet dataset with millions of $224\\times 224$ training images. The class names in one of the tasks there can be found [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). One of the most important breakthroughs was when in 2012 the convolutional neural network [AlexNet](https://en.wikipedia.org/wiki/AlexNet) won the first place. Ever since many highly successful convolutional neural networks architectures have been proposed, e.g. [VGG-16](https://arxiv.org/abs/1409.1556), [VGG-19](https://arxiv.org/abs/1409.1556), [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf), [Inception](https://arxiv.org/abs/1409.4842), etc. Training such networks requires a lot of time because they have many layers with millions of parameters. In this exercise we are going to experiment with pre-trained models of some of the best known architectures.\n",
        "\n",
        "### 8.2.1 Using pre-trained models\n",
        "Try running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qOJamRVpQrdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214c5b2b-3cf9-4618-c5b1-5f1b11557b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for resnet34:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:01<00:00, 76.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "badger: 99.5%\n",
            "wood rabbit: 89.4%\n",
            "sundial: 100.0%\n",
            "pineapple: 100.0%\n",
            "table lamp: 38.9%\n",
            "\n",
            "Results for vgg16:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:06<00:00, 79.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "badger: 100.0%\n",
            "wood rabbit: 98.9%\n",
            "sundial: 100.0%\n",
            "pineapple: 97.9%\n",
            "vase: 15.9%\n",
            "\n",
            "Results for vgg19:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:06<00:00, 82.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "badger: 100.0%\n",
            "wood rabbit: 97.3%\n",
            "sundial: 100.0%\n",
            "pineapple: 99.8%\n",
            "ashcan: 13.0%\n",
            "\n",
            "Results for inceptionv3:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:01<00:00, 75.5MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "badger: 95.4%\n",
            "wood rabbit: 90.6%\n",
            "sundial: 85.4%\n",
            "pineapple: 99.3%\n",
            "table lamp: 17.2%\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "from torchvision.io import read_image\n",
        "\n",
        "### Choose the architecture\n",
        "# architecture=\"resnet34\"\n",
        "#architecture=\"vgg16\"\n",
        "#architecture=\"vgg19\"\n",
        "#architecture=\"inceptionv3\"\n",
        "\n",
        "architectures = [\"resnet34\", \"vgg16\", \"vgg19\", \"inceptionv3\"]\n",
        "\n",
        "# Loop through each architecture\n",
        "for architecture in architectures:\n",
        "  print(f\"\\nResults for {architecture}:\\n\")\n",
        "\n",
        "  if architecture == \"resnet34\":\n",
        "      weights = models.ResNet34_Weights.DEFAULT\n",
        "      model = models.resnet34(weights=weights)\n",
        "  elif architecture == \"vgg16\":\n",
        "      weights = models.VGG16_Weights.DEFAULT\n",
        "      model = models.vgg16(pretrained=weights)\n",
        "  elif architecture == \"vgg19\":\n",
        "      weights = models.VGG19_Weights.DEFAULT\n",
        "      model = models.vgg19(pretrained=weights)\n",
        "  elif architecture == \"inceptionv3\":\n",
        "      weights = models.Inception_V3_Weights.DEFAULT\n",
        "      model = models.inception_v3(pretrained=weights)\n",
        "\n",
        "  model.eval()\n",
        "  image_paths=[\"/content/cnn_img/badger.jpg\", \"/content/cnn_img/rabbit.jpg\", \"/content/cnn_img/sundial.jpg\", \"/content/cnn_img/pineapple.jpg\", \"/content/cnn_img/can.jpg\"]\n",
        "\n",
        "  for path in image_paths:\n",
        "      #loading the image and rescaling it to fit the size for the imagenet architectures\n",
        "      img = read_image(path)\n",
        "      preprocess = weights.transforms(antialias=True)\n",
        "      batch = preprocess(img).unsqueeze(0)\n",
        "\n",
        "      prediction = model(batch).squeeze(0).softmax(0)\n",
        "      class_id = prediction.argmax().item()\n",
        "      score = prediction[class_id].item()\n",
        "      category_name = weights.meta[\"categories\"][class_id]\n",
        "\n",
        "      print(f\"{category_name}: {100 * score:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths=[\"/content/cnn_img/healthy/0.jpg\", \"/content/cnn_img/healthy/1.jpg\", \"/content/cnn_img/healthy/2.jpg\", \"/content/cnn_img/unhealthy/0.jpg\", \"/content/cnn_img/unhealthy/1.jpg\", \"/content/cnn_img/unhealthy/2.jpg\"]\n",
        "\n",
        "for architecture in architectures:\n",
        "  print(f\"\\nResults for {architecture}:\\n\")\n",
        "\n",
        "  if architecture == \"resnet34\":\n",
        "      weights = models.ResNet34_Weights.DEFAULT\n",
        "      model = models.resnet34(weights=weights)\n",
        "  elif architecture == \"vgg16\":\n",
        "      weights = models.VGG16_Weights.DEFAULT\n",
        "      model = models.vgg16(pretrained=weights)\n",
        "  elif architecture == \"vgg19\":\n",
        "      weights = models.VGG19_Weights.DEFAULT\n",
        "      model = models.vgg19(pretrained=weights)\n",
        "  elif architecture == \"inceptionv3\":\n",
        "      weights = models.Inception_V3_Weights.DEFAULT\n",
        "      model = models.inception_v3(pretrained=weights)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for path in image_paths:\n",
        "    #loading the image and rescaling it to fit the size for the imagenet architectures\n",
        "    img = read_image(path)\n",
        "    preprocess = weights.transforms(antialias=True)\n",
        "    batch = preprocess(img).unsqueeze(0)\n",
        "\n",
        "    prediction = model(batch).squeeze(0).softmax(0)\n",
        "    class_id = prediction.argmax().item()\n",
        "    score = prediction[class_id].item()\n",
        "    category_name = weights.meta[\"categories\"][class_id]\n",
        "\n",
        "    print(f\"{category_name}: {100 * score:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjwQQ5vj9LGq",
        "outputId": "898be6f0-359d-4af7-f968-ccf2576df969"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for resnet34:\n",
            "\n",
            "hot pot: 84.6%\n",
            "plate: 55.5%\n",
            "meat loaf: 44.2%\n",
            "potpie: 29.1%\n",
            "cheeseburger: 99.9%\n",
            "cheeseburger: 41.3%\n",
            "\n",
            "Results for vgg16:\n",
            "\n",
            "corn: 49.8%\n",
            "plate: 43.0%\n",
            "meat loaf: 27.4%\n",
            "carbonara: 21.2%\n",
            "cheeseburger: 88.8%\n",
            "cheeseburger: 36.2%\n",
            "\n",
            "Results for vgg19:\n",
            "\n",
            "corn: 23.9%\n",
            "cucumber: 49.0%\n",
            "bell pepper: 31.3%\n",
            "plate: 22.9%\n",
            "cheeseburger: 86.6%\n",
            "cheeseburger: 76.3%\n",
            "\n",
            "Results for inceptionv3:\n",
            "\n",
            "corn: 36.4%\n",
            "plate: 50.4%\n",
            "hamper: 29.7%\n",
            "cheeseburger: 35.3%\n",
            "cheeseburger: 97.6%\n",
            "carbonara: 29.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZmSyBglaUoa"
      },
      "source": [
        "**Tasks**\n",
        "1. Is there any significant difference between the results of different architectures?\n",
        "</br> Answer: Each architecture yields good results on some, and bad results on other objects at classification. Inceptionv3 may be the worst performing architecture (most insecure), from the listed, whereas ResNet performs fairly well and certain.\n",
        "2. Try to classify several other images from the folders cnn_img/healthy and cnn_img/unhealthy that you choose on your own. Which cases are problematic?\n",
        "</br> Answer: Every architecture except Resnet34 mistakenly \"hallucinated\" the existence of corn on the image, where there's none (it was hot pot, as correctly classified by Resnet34). resnet34 performed fairly well.\n",
        "\n",
        "### 8.2.2 Creating your own classifier - pincers vs. scissors\n",
        "Although ImageNet has a lot of classes, sometimes they do not cover some desired cases. Let's assume that we want to tell images with pincers apart from the ones with scissors. Neither pincers nor scissors are among ImageNet classes. Nevertheless, we can still use some parts of the pre-trained models.\n",
        "\n",
        "Various layers of a deep convolutional network have diferent tasks. The ones closest to the original input image usually look for features such as edges and corners i.e. for low-level features. After them there are layers that look for middle-level features such as circular objects, special curves, etc. Next, there are usually fully connected layers that create high-level semantic features by combining the information from the previous layers. These features are then used by the last layer that performs the actual classification. What we can do here is simply to discard the last layer i.e. not to calculate the class of an image, but to extract the values in on of the fully connected layers. This effectively means that we are going to use the network only as an extractor for high-level features that we would hardly be able to engineer on our own. Let's first see which layers can be found in the ResNet network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ztcpyruTRTJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa0b103-eea7-4fd6-cb0a-5680cf7ed022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "ReLU(inplace=True)\n",
            "MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (2): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (2): BasicBlock(\n",
            "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (3): BasicBlock(\n",
            "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (2): BasicBlock(\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (3): BasicBlock(\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (4): BasicBlock(\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (5): BasicBlock(\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (2): BasicBlock(\n",
            "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "Linear(in_features=512, out_features=1000, bias=True)\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "\n",
        "architecture=\"resnet34\"\n",
        "\n",
        "if architecture == \"resnet34\":\n",
        "  weights = models.ResNet34_Weights.DEFAULT\n",
        "  base_model = models.resnet34(weights=weights)\n",
        "elif architecture == \"resnet50\":\n",
        "  weights = models.ResNet50_Weights.DEFAULT\n",
        "  base_model = models.resnet50(weights=weights)\n",
        "elif architecture == \"vgg16\":\n",
        "  weights = models.VGG16_Weights.DEFAULT\n",
        "  base_model = models.vgg16(weights=weights)\n",
        "\n",
        "for layer in base_model.children():\n",
        "    print(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acvV9u06GocO"
      },
      "source": [
        "At the end you can see fully connected layer used for classification. We can extract the values from previous layers by using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Gxut2qMPJS3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08cca34c-f48a-48bd-ff0e-670734eb77cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 512])\n"
          ]
        }
      ],
      "source": [
        "# Model without last fully connected layer\n",
        "model = torch.nn.Sequential(*(list(base_model.children())[:-1]), nn.AdaptiveAvgPool2d(1))\n",
        "\n",
        "img_path=\"/content/cnn_img/rabbit.jpg\"\n",
        "\n",
        "img = read_image(img_path)\n",
        "preprocess = weights.transforms(antialias=True)\n",
        "batch = preprocess(img).unsqueeze(0)\n",
        "\n",
        "features = model(batch).squeeze(3).squeeze(2)\n",
        "\n",
        "print(features.shape)\n",
        "feature_layer_size=features.shape[1];"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjTMbaFPKfvk"
      },
      "source": [
        "These values can now be used as features and that can later be used with another classifier. Let's first extract the features for our pincer and scissors images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WPNycPJ4KhBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a0b940-9ff8-4d75-f7dc-545925e08620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating training features...\n",
            "\t 1 / 50\n",
            "\t 2 / 50\n",
            "\t 3 / 50\n",
            "\t 4 / 50\n",
            "\t 5 / 50\n",
            "\t 6 / 50\n",
            "\t 7 / 50\n",
            "\t 8 / 50\n",
            "\t 9 / 50\n",
            "\t10 / 50\n",
            "\t11 / 50\n",
            "\t12 / 50\n",
            "\t13 / 50\n",
            "\t14 / 50\n",
            "\t15 / 50\n",
            "\t16 / 50\n",
            "\t17 / 50\n",
            "\t18 / 50\n",
            "\t19 / 50\n",
            "\t20 / 50\n",
            "\t21 / 50\n",
            "\t22 / 50\n",
            "\t23 / 50\n",
            "\t24 / 50\n",
            "\t25 / 50\n",
            "\t26 / 50\n",
            "\t27 / 50\n",
            "\t28 / 50\n",
            "\t29 / 50\n",
            "\t30 / 50\n",
            "\t31 / 50\n",
            "\t32 / 50\n",
            "\t33 / 50\n",
            "\t34 / 50\n",
            "\t35 / 50\n",
            "\t36 / 50\n",
            "\t37 / 50\n",
            "\t38 / 50\n",
            "\t39 / 50\n",
            "\t40 / 50\n",
            "\t41 / 50\n",
            "\t42 / 50\n",
            "\t43 / 50\n",
            "\t44 / 50\n",
            "\t45 / 50\n",
            "\t46 / 50\n",
            "\t47 / 50\n",
            "\t48 / 50\n",
            "\t49 / 50\n",
            "\t50 / 50\n",
            "Creating test features...\n",
            "\t 1 / 50\n",
            "\t 2 / 50\n",
            "\t 3 / 50\n",
            "\t 4 / 50\n",
            "\t 5 / 50\n",
            "\t 6 / 50\n",
            "\t 7 / 50\n",
            "\t 8 / 50\n",
            "\t 9 / 50\n",
            "\t10 / 50\n",
            "\t11 / 50\n",
            "\t12 / 50\n",
            "\t13 / 50\n",
            "\t14 / 50\n",
            "\t15 / 50\n",
            "\t16 / 50\n",
            "\t17 / 50\n",
            "\t18 / 50\n",
            "\t19 / 50\n",
            "\t20 / 50\n",
            "\t21 / 50\n",
            "\t22 / 50\n",
            "\t23 / 50\n",
            "\t24 / 50\n",
            "\t25 / 50\n",
            "\t26 / 50\n",
            "\t27 / 50\n",
            "\t28 / 50\n",
            "\t29 / 50\n",
            "\t30 / 50\n",
            "\t31 / 50\n",
            "\t32 / 50\n",
            "\t33 / 50\n",
            "\t34 / 50\n",
            "\t35 / 50\n",
            "\t36 / 50\n",
            "\t37 / 50\n",
            "\t38 / 50\n",
            "\t39 / 50\n",
            "\t40 / 50\n",
            "\t41 / 50\n",
            "\t42 / 50\n",
            "\t43 / 50\n",
            "\t44 / 50\n",
            "\t45 / 50\n",
            "\t46 / 50\n",
            "\t47 / 50\n",
            "\t48 / 50\n",
            "\t49 / 50\n",
            "\t50 / 50\n"
          ]
        }
      ],
      "source": [
        "def create_numbered_paths(home_dir, n):\n",
        "    return [home_dir+str(i)+\".jpg\" for i in range(n)]\n",
        "\n",
        "def create_paired_numbered_paths(first_home_dir, second_home_dir, n):\n",
        "    image_paths=[]\n",
        "    for p in zip(create_numbered_paths(first_home_dir, n), create_numbered_paths(second_home_dir, n)):\n",
        "        image_paths.extend(p)\n",
        "    return image_paths\n",
        "\n",
        "def create_features(paths, verbose=True):\n",
        "    n=len(paths)\n",
        "    features=np.zeros((n, feature_layer_size))\n",
        "    for i in range(n):\n",
        "        if (verbose==True):\n",
        "            print(\"\\t%2d / %2d\"%(i+1, n))\n",
        "        img = read_image(paths[i])\n",
        "        preprocess = weights.transforms(antialias=True)\n",
        "        batch = preprocess(img).unsqueeze(0)\n",
        "        features[i, :]=model(batch).squeeze(3).squeeze(2).detach().numpy()\n",
        "\n",
        "    return features\n",
        "\n",
        "pincers_dir=\"/content/cnn_img/pincers/\"\n",
        "scissors_dir=\"/content/cnn_img/scissors/\"\n",
        "\n",
        "individual_n=50\n",
        "\n",
        "#combining all image paths\n",
        "image_paths=create_paired_numbered_paths(pincers_dir, scissors_dir, individual_n)\n",
        "\n",
        "#marking their classes\n",
        "image_classes=[]\n",
        "for i in range(individual_n):\n",
        "    #0 stands for the pincer image and 0 stands for the scissors image\n",
        "    image_classes.extend((0, 1))\n",
        "\n",
        "#number of all images\n",
        "n=100\n",
        "#number of training images\n",
        "n_train=50\n",
        "#number of test images\n",
        "n_test=n-n_train\n",
        "\n",
        "print(\"Creating training features...\")\n",
        "#here we will store the features of training images\n",
        "x_train=create_features(image_paths[:n_train])\n",
        "#train classes\n",
        "y_train=np.array(image_classes[:n_train])\n",
        "\n",
        "print(\"Creating test features...\")\n",
        "#here we will store the features of test images\n",
        "x_test=create_features(image_paths[n_train:])\n",
        "\n",
        "#train classes\n",
        "y_test=np.array(image_classes[n_train:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQQp23pwweEf"
      },
      "source": [
        "Now that for each image we have its features, we will divide the images into a training and a test set. Then we will use a linear SVM classifier to classify them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "R12r9KSrgt95"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "def create_svm_classifier(x, y, C=1.0, kernel='linear'):\n",
        "    #we will use linear SVM\n",
        "    classifier=svm.SVC(kernel=kernel, C=C);\n",
        "    classifier.fit(x, y)\n",
        "    return classifier\n",
        "\n",
        "def calculate_accuracy(classifier, x, y):\n",
        "    predicted=classifier.predict(x)\n",
        "    return np.sum(y==predicted)/y.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ADOM91TWwdmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba43b827-c645-46b3-f4cb-91a063919f69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Complex SVM): 98.00%\n",
            "Accuracy (Linear SVM): 96.00%\n"
          ]
        }
      ],
      "source": [
        "# Experiment with different SVM models\n",
        "complex_classifier = create_svm_classifier(x_train, y_train, C=1.0, kernel='rbf')\n",
        "linear_classifier = create_svm_classifier(x_train, y_train, C=1.0, kernel='linear')\n",
        "\n",
        "# Calculate and print accuracies for different models\n",
        "accuracy_complex = calculate_accuracy(complex_classifier, x_test, y_test)\n",
        "accuracy_linear = calculate_accuracy(linear_classifier, x_test, y_test)\n",
        "\n",
        "print(f\"Accuracy (Complex SVM): {100 * accuracy_complex:.2f}%\")\n",
        "print(f\"Accuracy (Linear SVM): {100 * accuracy_linear:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSMhoT6swkeZ"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. Is there any significant gain if more complex SVM models are used?\n",
        "</br> Answer: yes there is, a whole 2% gain in accuracy when using rbf kernel as compared to linear kernel, bringing the accuracy from 96% to 98%.\n",
        "2. What happens if we extract features using different backbone, e.g. vgg16? </br> Answer: Well, the vgg16 backbone yields 100% accuracy on given data and task\n",
        "\n",
        "\n",
        "### 8.2.1 Creating your own classifier - healthy vs. unhealthy food\n",
        "The previous example was relatively simple because all images were of same size and each of them had a white background, which allowed the extractor to concentrate only on the features of the actual objects. In this example we will use a slightly more complicated case - namely, will will tell images with healthy food apart from the ones with unhealthy food. FIrst let's repeat the same process as we did in the previous example and create the features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "JI75TDrVxNrR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0dde347-8f35-40d7-9587-f56fc097bcea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating training features...\n",
            "\t 1 / 100\n",
            "\t 2 / 100\n",
            "\t 3 / 100\n",
            "\t 4 / 100\n",
            "\t 5 / 100\n",
            "\t 6 / 100\n",
            "\t 7 / 100\n",
            "\t 8 / 100\n",
            "\t 9 / 100\n",
            "\t10 / 100\n",
            "\t11 / 100\n",
            "\t12 / 100\n",
            "\t13 / 100\n",
            "\t14 / 100\n",
            "\t15 / 100\n",
            "\t16 / 100\n",
            "\t17 / 100\n",
            "\t18 / 100\n",
            "\t19 / 100\n",
            "\t20 / 100\n",
            "\t21 / 100\n",
            "\t22 / 100\n",
            "\t23 / 100\n",
            "\t24 / 100\n",
            "\t25 / 100\n",
            "\t26 / 100\n",
            "\t27 / 100\n",
            "\t28 / 100\n",
            "\t29 / 100\n",
            "\t30 / 100\n",
            "\t31 / 100\n",
            "\t32 / 100\n",
            "\t33 / 100\n",
            "\t34 / 100\n",
            "\t35 / 100\n",
            "\t36 / 100\n",
            "\t37 / 100\n",
            "\t38 / 100\n",
            "\t39 / 100\n",
            "\t40 / 100\n",
            "\t41 / 100\n",
            "\t42 / 100\n",
            "\t43 / 100\n",
            "\t44 / 100\n",
            "\t45 / 100\n",
            "\t46 / 100\n",
            "\t47 / 100\n",
            "\t48 / 100\n",
            "\t49 / 100\n",
            "\t50 / 100\n",
            "\t51 / 100\n",
            "\t52 / 100\n",
            "\t53 / 100\n",
            "\t54 / 100\n",
            "\t55 / 100\n",
            "\t56 / 100\n",
            "\t57 / 100\n",
            "\t58 / 100\n",
            "\t59 / 100\n",
            "\t60 / 100\n",
            "\t61 / 100\n",
            "\t62 / 100\n",
            "\t63 / 100\n",
            "\t64 / 100\n",
            "\t65 / 100\n",
            "\t66 / 100\n",
            "\t67 / 100\n",
            "\t68 / 100\n",
            "\t69 / 100\n",
            "\t70 / 100\n",
            "\t71 / 100\n",
            "\t72 / 100\n",
            "\t73 / 100\n",
            "\t74 / 100\n",
            "\t75 / 100\n",
            "\t76 / 100\n",
            "\t77 / 100\n",
            "\t78 / 100\n",
            "\t79 / 100\n",
            "\t80 / 100\n",
            "\t81 / 100\n",
            "\t82 / 100\n",
            "\t83 / 100\n",
            "\t84 / 100\n",
            "\t85 / 100\n",
            "\t86 / 100\n",
            "\t87 / 100\n",
            "\t88 / 100\n",
            "\t89 / 100\n",
            "\t90 / 100\n",
            "\t91 / 100\n",
            "\t92 / 100\n",
            "\t93 / 100\n",
            "\t94 / 100\n",
            "\t95 / 100\n",
            "\t96 / 100\n",
            "\t97 / 100\n",
            "\t98 / 100\n",
            "\t99 / 100\n",
            "\t100 / 100\n",
            "Creating test features...\n",
            "\t 1 / 100\n",
            "\t 2 / 100\n",
            "\t 3 / 100\n",
            "\t 4 / 100\n",
            "\t 5 / 100\n",
            "\t 6 / 100\n",
            "\t 7 / 100\n",
            "\t 8 / 100\n",
            "\t 9 / 100\n",
            "\t10 / 100\n",
            "\t11 / 100\n",
            "\t12 / 100\n",
            "\t13 / 100\n",
            "\t14 / 100\n",
            "\t15 / 100\n",
            "\t16 / 100\n",
            "\t17 / 100\n",
            "\t18 / 100\n",
            "\t19 / 100\n",
            "\t20 / 100\n",
            "\t21 / 100\n",
            "\t22 / 100\n",
            "\t23 / 100\n",
            "\t24 / 100\n",
            "\t25 / 100\n",
            "\t26 / 100\n",
            "\t27 / 100\n",
            "\t28 / 100\n",
            "\t29 / 100\n",
            "\t30 / 100\n",
            "\t31 / 100\n",
            "\t32 / 100\n",
            "\t33 / 100\n",
            "\t34 / 100\n",
            "\t35 / 100\n",
            "\t36 / 100\n",
            "\t37 / 100\n",
            "\t38 / 100\n",
            "\t39 / 100\n",
            "\t40 / 100\n",
            "\t41 / 100\n",
            "\t42 / 100\n",
            "\t43 / 100\n",
            "\t44 / 100\n",
            "\t45 / 100\n",
            "\t46 / 100\n",
            "\t47 / 100\n",
            "\t48 / 100\n",
            "\t49 / 100\n",
            "\t50 / 100\n",
            "\t51 / 100\n",
            "\t52 / 100\n",
            "\t53 / 100\n",
            "\t54 / 100\n",
            "\t55 / 100\n",
            "\t56 / 100\n",
            "\t57 / 100\n",
            "\t58 / 100\n",
            "\t59 / 100\n",
            "\t60 / 100\n",
            "\t61 / 100\n",
            "\t62 / 100\n",
            "\t63 / 100\n",
            "\t64 / 100\n",
            "\t65 / 100\n",
            "\t66 / 100\n",
            "\t67 / 100\n",
            "\t68 / 100\n",
            "\t69 / 100\n",
            "\t70 / 100\n",
            "\t71 / 100\n",
            "\t72 / 100\n",
            "\t73 / 100\n",
            "\t74 / 100\n",
            "\t75 / 100\n",
            "\t76 / 100\n",
            "\t77 / 100\n",
            "\t78 / 100\n",
            "\t79 / 100\n",
            "\t80 / 100\n",
            "\t81 / 100\n",
            "\t82 / 100\n",
            "\t83 / 100\n",
            "\t84 / 100\n",
            "\t85 / 100\n",
            "\t86 / 100\n",
            "\t87 / 100\n",
            "\t88 / 100\n",
            "\t89 / 100\n",
            "\t90 / 100\n",
            "\t91 / 100\n",
            "\t92 / 100\n",
            "\t93 / 100\n",
            "\t94 / 100\n",
            "\t95 / 100\n",
            "\t96 / 100\n",
            "\t97 / 100\n",
            "\t98 / 100\n",
            "\t99 / 100\n",
            "\t100 / 100\n"
          ]
        }
      ],
      "source": [
        "healthy_dir=\"/content/cnn_img/healthy/\"\n",
        "unhealthy_dir=\"/content/cnn_img/unhealthy/\"\n",
        "\n",
        "individual_n=100\n",
        "\n",
        "#combining all image paths\n",
        "image_paths=create_paired_numbered_paths(healthy_dir, unhealthy_dir, individual_n)\n",
        "\n",
        "#marking their classes\n",
        "image_classes=[]\n",
        "for i in range(individual_n):\n",
        "    #0 stands for the pincer image and 0 stands for the scissors image\n",
        "    image_classes.extend((0, 1))\n",
        "\n",
        "#number of all images\n",
        "n=200\n",
        "#number of training images\n",
        "n_train=100\n",
        "#number of test images\n",
        "n_test=n-n_train\n",
        "\n",
        "print(\"Creating training features...\")\n",
        "#here we will store the features of training images\n",
        "x_train=create_features(image_paths[:n_train])\n",
        "#train classes\n",
        "y_train=np.array(image_classes[:n_train])\n",
        "\n",
        "print(\"Creating test features...\")\n",
        "#here we will store the features of test images\n",
        "x_test=create_features(image_paths[n_train:])\n",
        "#train classes\n",
        "y_test=np.array(image_classes[n_train:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfezje2qxQns"
      },
      "source": [
        "Now let's train a model and test its accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TXUOBrkjxNzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481e2e5f-c5f6-4ca2-dd33-f5c9753a50a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 65.00%\n"
          ]
        }
      ],
      "source": [
        "classifier=create_svm_classifier(x_train, y_train, 1.0, \"linear\")\n",
        "print(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Zt5OzoxWqj"
      },
      "source": [
        "**Tasks**\n",
        "1. Try the whole food classification with another network as a feature extractor and compare their results.\n",
        "</br> Answer: Vgg16 yields accuracy of 92%, compared to 65% accuracy of resnet34.\n",
        "2. What kind of test images are problematic?\n",
        "</br> Answer: the problematic test images are usually ones with multiple objects and foods present in a single image, hence confusing the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.image as mpimg\n",
        "\n",
        "# # Function to visualize misclassified images\n",
        "# def visualize_misclassified(classifier, x, y, paths):\n",
        "#     predicted = classifier.predict(x)\n",
        "#     misclassified_indices = np.where(y != predicted)[0]\n",
        "\n",
        "#     for idx in misclassified_indices:\n",
        "#         img_path = paths[idx]\n",
        "#         true_label = \"Healthy\" if y[idx] == 0 else \"Unhealthy\"\n",
        "#         predicted_label = \"Healthy\" if predicted[idx] == 0 else \"Unhealthy\"\n",
        "\n",
        "#         # Load and display the misclassified image\n",
        "#         img = mpimg.imread(img_path)\n",
        "#         plt.imshow(img)\n",
        "#         plt.title(f\"True Label: {true_label}, Predicted Label: {predicted_label}\")\n",
        "#         plt.axis('off')\n",
        "#         plt.show()\n",
        "\n",
        "# # Visualize misclassified images for the original ResNet model\n",
        "# visualize_misclassified(classifier, x_test, y_test, image_paths[n_train:])\n"
      ],
      "metadata": {
        "id": "AqnfogBDfK1A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}